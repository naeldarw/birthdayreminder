{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPFXjAVFIKnh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import platform\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uGxnwhvlwMiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04ad643-a5c4-42b3-f197-b902301341b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vita-epfl/DLAV-2025.git\n",
        "path = os.getcwd() + '/DLAV-2025/homeworks/hw2/test_batch'"
      ],
      "metadata": {
        "id": "SwxcJW9wI9fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3516352-b84f-43fe-f256-5bea45f5a7ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLAV-2025'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 47 (delta 7), reused 6 (delta 6), pack-reused 32 (from 1)\u001b[K\n",
            "Receiving objects: 100% (47/47), 27.72 MiB | 13.55 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n",
        "### Path to Model Weights\n",
        "pytorch_weights = os.getcwd() + '/drive/MyDrive/Colab Notebooks/linearClassifier_pytorch.ckpt'\n",
        "softmax_weights = os.getcwd() + '/drive/MyDrive/Colab Notebooks/softmax_weights.pkl'"
      ],
      "metadata": {
        "id": "pZXQTJIKJE_S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy your code from the Softmax Notebook to their corresponding function"
      ],
      "metadata": {
        "id": "mE6psT_aVPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    y_pred = X @ W\n",
        "    y_pred -= np.max(y_pred, axis=1, keepdims=True)\n",
        "    y_pred = np.exp(y_pred)\n",
        "    y_pred /= np.sum(y_pred, axis=1, keepdims=True)\n",
        "    loss = -np.sum(np.log(y_pred[range(X.shape[0]), y]))\n",
        "    loss /= X.shape[0]\n",
        "    y_pred[range(X.shape[0]), y] -= 1\n",
        "    dW = X.T @ y_pred\n",
        "    dW /= X.shape[0]\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n",
        "                batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            indices = np.random.choice(num_train, batch_size)\n",
        "            X_batch = X[indices]\n",
        "            y_batch = y[indices]\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W -= learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        y_pred = X @ self.W\n",
        "        y_pred -= np.max(y_pred, axis=1, keepdims=True)\n",
        "        y_pred = np.exp(y_pred)\n",
        "        y_pred /= np.sum(y_pred, axis=1, keepdims=True)\n",
        "        # print(f\"Softmax probs are {y_pred}\")\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "\n",
        "         e = y_batch - np.dot(X_batch, self.W)\n",
        "\n",
        "        loss = np.dot(e.T, e)\n",
        "        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n",
        "\n",
        "        return loss, grad\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
      ],
      "metadata": {
        "id": "gHnLX6-oIkWm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy the model you created from the Pytorch Notebook"
      ],
      "metadata": {
        "id": "6chaH4G-Vfms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.fc2 = torch.nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # !!New method added from the Pytorch Notebook\n",
        "    def predict(self, x):\n",
        "      logits = self.forward(x)\n",
        "      return F.softmax(logits)"
      ],
      "metadata": {
        "id": "mSTfKTHEJBhy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n",
        "\n"
      ],
      "metadata": {
        "id": "_UUbNTUAVsos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_usingPytorch(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Create your model                                                   #\n",
        "    # - Load your saved model                                               #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n",
        "    #########################################################################\n",
        "    net = Net(3072, 600, 10)\n",
        "    net.load_state_dict(torch.load(pytorch_weights))\n",
        "    # y_pred = np.array(range(X.size(0)))\n",
        "    probs = net.predict(X)\n",
        "    print(f\"pytorch probs are {probs}\")\n",
        "    y_pred = torch.argmax(probs, dim=1)\n",
        "    print(f\"pytorch prediction is {y_pred}\")\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred.numpy()\n",
        "\n",
        "def predict_usingSoftmax(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Load your saved model into the weights of Softmax                   #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array                                 #\n",
        "    #########################################################################\n",
        "    with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n",
        "      W = pickle.load(f)\n",
        "    # print(f\"softmax weights are {W}\")\n",
        "    new_softmax = Softmax()\n",
        "    new_softmax.W = W.copy()\n",
        "    y_pred = new_softmax.predict(X)\n",
        "    # print(f\"softmax prediction is {y_pred}\")\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "bEKafMuaI4By"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method loads the test dataset to evaluate the model."
      ],
      "metadata": {
        "id": "q8dM8fj39OBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Read DATA\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "test_filename = path\n",
        "X,Y = load_CIFAR_batch(test_filename)"
      ],
      "metadata": {
        "id": "400u4eZNJAZq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here."
      ],
      "metadata": {
        "id": "AJ3mBYnx9TIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Manipulation\n",
        "\n",
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std = np.array([0.2023, 0.1994, 0.2010])\n",
        "X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "\n",
        "X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n",
        "X_softmax = np.reshape(X, (X.shape[0], -1))\n",
        "X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"
      ],
      "metadata": {
        "id": "IEmU5KnwJPBY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_pytorch.shape)\n",
        "print(X_softmax.shape)"
      ],
      "metadata": {
        "id": "swJrTtFqIBd_",
        "outputId": "22e0bff0-d4ea-4277-d827-cb0380e94973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 3, 32, 32])\n",
            "(10000, 3073)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O2nQbKPL9c3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run Prediction\n",
        "prediction_pytorch = predict_usingPytorch(X_pytorch)\n",
        "prediction_softmax = predict_usingSoftmax(X_softmax)\n",
        "\n",
        "## Run Evaluation\n",
        "acc_softmax = sum(prediction_softmax == Y)/len(X)\n",
        "acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n",
        "print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"
      ],
      "metadata": {
        "id": "VKFPhm1wJjDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304d4319-cb00-4dbb-d728-f341b8f86bee"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-556fcee8a30d>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(pytorch_weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch probs are tensor([[0.0315, 0.0351, 0.3518,  ..., 0.0191, 0.0032, 0.0057],\n",
            "        [0.0338, 0.0368, 0.3556,  ..., 0.0194, 0.0033, 0.0064],\n",
            "        [0.0339, 0.0358, 0.3562,  ..., 0.0193, 0.0033, 0.0062],\n",
            "        ...,\n",
            "        [0.0303, 0.0325, 0.3535,  ..., 0.0193, 0.0030, 0.0054],\n",
            "        [0.0314, 0.0344, 0.3510,  ..., 0.0195, 0.0031, 0.0057],\n",
            "        [0.0310, 0.0339, 0.3524,  ..., 0.0202, 0.0030, 0.0057]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "pytorch prediction is tensor([2, 2, 2,  ..., 2, 2, 2])\n",
            "Softmax probs are [[0.02398246 0.26096146 0.03513365 ... 0.11211585 0.02538142 0.10318027]\n",
            " [0.02469627 0.26706417 0.03443716 ... 0.11286426 0.02619485 0.11087338]\n",
            " [0.02512423 0.2637516  0.03514134 ... 0.11492727 0.02648502 0.10965312]\n",
            " ...\n",
            " [0.02401481 0.25141787 0.03441598 ... 0.11726684 0.02451482 0.10397931]\n",
            " [0.02362384 0.256266   0.03455883 ... 0.11477297 0.02485136 0.10531852]\n",
            " [0.02421012 0.25407564 0.03528871 ... 0.11977435 0.02470398 0.10500255]]\n",
            "Softmax= 0.100000 ... Pytorch= 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-43fc3b756d4b>:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(logits)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qroI8swROjZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}